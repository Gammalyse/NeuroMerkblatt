\documentclass[a4paper,ngerman,10pt]{article}

\usepackage{ucs}
\usepackage[utf8]{inputenc}
\usepackage{calc}
\usepackage{fixltx2e}
\usepackage[ngerman]{babel}
\usepackage[dvipsnames, table,rgb]{xcolor}
\usepackage{fontenc}
\usepackage{graphicx}
\usepackage[]{graphicx}
\usepackage[left=2.5cm, right=2.5cm, top=3cm, bottom=4cm]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{cite}
% \usepackage{epstopdf}
\usepackage{float}
% \usepackage{flafter}
% \usepackage[section]{placeins}
% \usepackage{caption}
% \usepackage{subcaption}
% \usepackage[fleqn]{amsmath}
% \usepackage[all, error]{onlyamsmath}
% \usepackage{booktabs}
% \usepackage{ltxtable}
% \usepackage{url}
% \usepackage{enumitem}
% \usepackage{ragged2e}
% \usepackage{tikz}
% \usepackage{pgf}
% \usepackage{setspace}
% \usepackage[automark, komastyle, nouppercase]{scrpage2}
% \usepackage{pdfpages}
% \usepackage{microtype}
% \usepackage{listliketab}
% \usepackage{booktabs}
% \usepackage{verbatim}
% \usepackage{multirow}
% \usepackage{todonotetes}
% \usepackage{tikz}
% \usepackage{subfigtes}
% \usepackage{tikz}
% \usepackage{subfigs}
% \usepackage{tikz}
% \usepackage{subfig}
% \usepackage{listingsutf8}
% \usepackage{color}
% \usepackage{amsmath}
% \usepackage[titletoc]{appendix}
% \usepackage[xindy]{imakeidx}
% \makeindex
% \setlength{\mathindent}{0pt}
\newcommand*\mean[1]{\overline{#1}}
% \numberwithin{equation}{subsection}
\date{01/23/15}

\begin{document}

    \tableofcontents
    \newpage
    \section{Formeln}
        \begin{itemize}
        \item Hodgkin-Huxley (Hodgkin 1952):
            \begin{subequations}
                \begin{flalign}
                C_m\frac{\mathrm d}{\mathrm d t}V_m &= I
                -\overbrace{g_Kn^4(V_m-E_K)}^\text{$I_K$}
                -\overbrace{g_{Na}m^3h(V_m-E_{Na})}^\text{$I_{Na}$}
                -\overbrace{g_L(V_m-E_L)}^\text{$I_L$}&\\
                \dot n &= (n_\infty(V_m)-n)/\tau_n(V_m)&\\
                \nonumber\dot m &= (m_\infty(V_m)-m)/\tau_m(V_m)&\\
                \nonumber\dot h &= (h_\infty(V_m)-h)/\tau_h(V_m)&
                \end{flalign}
            \end{subequations}
        \item Axon-Hillock IAF (Mead 1989):
            \begin{flalign}
            C_{m}\frac{\mathrm d}{\mathrm d t}V_{m} &= I_{in}(t)-\beta,
            \quad V_m(t)\in(0,\varTheta)&
            \end{flalign}
        \item CLIAF (Neftci 2011):
            \begin{flalign}
            C_{m}\frac{\mathrm d}{\mathrm d t}V_{m} &= I_{in}(t)-\beta+I_{fb}e^
            {\frac{\kappa}{U_{T}}(V_{m}-V_{th})}&
            \end{flalign}
        \item Quadratic IAF (Gerstner 2002):
            \begin{flalign}
            \tau_{m}\frac{\mathrm d}{\mathrm d t}V_{m} &= a_{0}(u-u_{rest})(u-
            u_{thresh})+RI,\quad a_{0}>0&
            \end{flalign}
        \item Izhikevich (Izhikevich 2003):
            \begin{subequations}
                \begin{flalign}
                \dot V_{m} &= 0.04 V_{m}^2 + 5 V_{m} + 140 - u + I &\\
                \dot u &= a(b V_m - u)&
                \end{flalign}
            \end{subequations}
        \item AdEx IAF (Brette 2005):
            \begin{subequations}
                \begin{flalign}
                -C_{m}\frac{\mathrm d}{\mathrm d t}V_{m} &= g_1(V_m-E_1)-g_1
                \Delta_te^{\frac{V_m-V_t}{\Delta_t}}+w&\\
                \nonumber&\qquad+g_e(t)(V_m-E_e)&\\
                \nonumber&\qquad+g_i(t)(V_m-E_i)&\\
                -\tau_w\frac{\mathrm d}{\mathrm d t}w &= w-a(V_m-E_1),\quad
                \end{flalign}
            \end{subequations}
        \end{itemize}

    \section{Einheiten}
        \begin{itemize}
        \item $\beta$ - Konstanter Leckstrom
        \item $I_{in}(t)$ - Neuronaler Inputstrom
        \end{itemize}

    \section{Abkürzungen}
        AER - Address Event Representation\\
        CCN - Cooperative and Competitive Network\\
        CLIAF - Constant Leaky Integrate And Fire neuron\\
        CV - Coefficient of Variation\\
        DAC - Digital to Analog Converter\\
        DAP - Depolarizing After-Potential\\
        DNC - Digital Network Chip\\
        DPI - Differential Pair Integrator\\
        EPSC - Excitatory Postsynaptic Current\\
        FPGA - Field Programmable Gate Array\\
        HH - Hodgkin Huxley neuron model\\
        HICANN - High Input Count Analog Neural Network\\
        IAF - Integrate-and-fire neuron model\\
        ISI - InterSpike Intervall\\
        LTD - Long Term Depression\\
        LTP - Long Term Potention\\
        LTU - Linear Threshold Unit\\
        ODE - Ordinary Differential Equation\\
        SAC - Selective Attention Chip\\
        SSM - Soft State Machine\\
        STDP - Spike Timing Dependent Plasticity\\
        sWTA - Soft Winner-Take-All Network

    \section{Definitionen}
        \begin{itemize}
        \item AER: Communication Protocoll which describes spikes from sources
        \item overconstrained: equations outnumber the unknowns
        \item raster plot: spikes of neurons plotted in time
        \end{itemize}

    \section{Bekannte Forscher}
        \begin{description}
        \item[University of Lorraine] Laure Buhry (HH parameter estimation)
        \item[La Jolla] Emre Neftci
        \item[Bielefeld] Elisabetta Chicca, Rodney J. Douglas
        \item[Zürich] Giacomo Indiveri, Christian G Mayr
        \item[Heidelberg] Karlheinz Meier, Daniel Brüderle. Johannes Schemmel,
              Thomas Pfeil
        \item[Stockholm] Anders Lansner, Jeanette Hellgren Kotaleski
        \item[Stockholm INCF] Sean L Hill
        \item[Stanford] Kwabena Boahen, Peiran Gao
        \item[Gif-sur-Yvette CNRS] Andrew P Davison (pyNN)
        \item[Talence] Sylvie Renaud
        \item[Lausanne EPFL] Henry Makram, Eilif Benjamin Muller, Wulfram
             Gerstner, Marc-Oliver Gewaltig
        \item[Jülich] Markus Diesmann, Moritz Helias, Jochen Martin Eppler
        \item[Oslo] Jan G Bjaalie (Neuroinformatics)
        \item[Edinburgh] James A Bednar
        \item[New Haven, Yale] Michael Hines
        \item[Other] Carver Mead, Misha Maowald, Lynn Conway
        \item[San Francisco] Eugene M. Izhikevich, Todd Hylton
        \end{description}

    \section{Notizen zu wichtigen Arbeiten}

        \subsection{Neftci 2010: A Device Mismatch Compensation Method for VLSI
            Neural Networks}
            \begin{itemize}
            \item motivation: transistor properties mismatch as major VLSI
                  problem
            \item mismatch compensation algorithm through connectivity
            \item no layout disadvantage (no extra HW memory)
            \item exploits Address-Event Representation
            \item metaplasticity as homeostatic mechanism for homogeneous
                  population response
            \item metaplasticity: change connectivity profile (\# or \%) to
                  normalize response strength
            \item test: sWTA as possible general purpose structure
            \item synaptic scaling method to normalize response of VLSI sWTA with
                  HW respresentation and constraints (e.g. spiking output)
            \item synaptic circuits with linear filter (1st order) to summarize
                  different sources
            \item 1 synapse emulates synapses with same time constant and weight
                  $\rightarrow$ modulates coupling strength between populations
            \item test of theory with sWTA and an compensation matrix
                  $\rightarrow$ up to 40\% less variability and same mean in
                  transfer function
            \item fig 1: bumps getting homogeneous after compensation (reduced
                  mismatch)
            \item increased discrimination performance: generally better win rate
                  of stronger input population
            \end{itemize}

        \subsection{Neftci 2011: A Systematic Method for Configuring VLSI
            Networks of Spiking Neurons}
            \begin{itemize}
            \item motivation: subthreshold transistor $\rightarrow$ small signal
                  $\rightarrow$ prone to noise and mismatch
            \item motivation: high-lvl program language needed $\rightarrow$
                  auto determining of biases taking mismatch in account
            \item motivation: determining bias voltages corresponding to model
                  params through iterative search $\rightarrow$ resource and
                  time consuming \& no predictive power (must be executed after
                  every param change)
            \item automatic bidirectional parameter mapping technique (high-level
                  NN simulation $\leftarrow\rightarrow$ VLSI)
            \item algorithm is general and usable if LTU behaviour approximated
                  in circuit (IAF for example) / if regime that can be modeled
                  using mean-field approach
            \item approximation possible if a regime exists in which neurons have
                  threshold-activation linear function (e.g. CLIAF)
            \item algorithm: standart parameter translation + determining bias
                  voltage (one calibration for futuer uses)
            \item LTU $\leftarrow\rightarrow$ CLIAF model $\leftarrow\rightarrow$
                  VLSI spiking neuron
            \item drawback: to calibrate and build parameter translation detailed
                  knowledge of circuit needed
            \item drawback: LTU approximation only works if $\tau_{syn}\gg
                  T_{ISI}$
            \item Equations (2.5)+(2.6) as complete approximated description
            \item Equation (2.7)+(2.8) as bridge between LTUs and silicon neurons
            \item Equation (2.9)+(2.8) as specified CCN transfer output
            \item LTU approximation is accurate for setting gain and predicting
                  steady state mean firing rate
            \item $\tau$ of hardware and software identical
            \item exact VLSI sWTA tuning possible
            \end{itemize}

        \subsection{Neftci 2013: Synthesizing cognition in neuromorphic
            electronic systems}
            \begin{itemize}
            \item motivation: reliable behavioural dynamics of noisy neurons
                  needed
            \item neuromorphic agent as autonomous entity, observes (sensors)
                  and acts (effectors) on context/stimuli, strives for abstract
                  goal
            \item behavioural dynamics needed instead of programming logic
            \item 3 layers: high-level behaviour model, abstract computational
                  (sWTA), analog-digital hardware
            \item (1) mapping hardware $\rightarrow$ abstract model (2) create
                  sWTA (3) create sparse connections between populations to form
                  soft state machine (SSM)
            \item reliable state processing through active gain, signal
                  restoration (memory), multistability (binary)
            \item 3 sWTA networks: state, state mapping, output through mapping
            \item visual task: detect in visual field horizontal bar
                  L$\rightarrow$R or vertical bar R$\rightarrow$L and give output
            \item visual field is seperated into left, right, 2 borders and 2
                  cues
            \item (1) silicon retina detects signal in cue and decides feature
                  map (horizontal LR or vertical RL)
            \item (2) feature map events mapped to selective attention chip (SAC)
            \item (3) SAC activates spiking neuron on visual field position
            \item (4) SSM reacts with state, transition and output sWTA
                  stimulation
            \item paper shows simplifying programming of high-lvl behaviour and
                  reliable processing
            \end{itemize}

        \subsection{Sheik 2011: Systematic configuration and automatic tuning of
            neuromorphic Systems}
            \begin{itemize}
            \item relationship between parameters model, software, hardware
                  non-linear
            \item modular configuration framework for multi-chip neuromorphic
                  VLSI and automatic mapping from parameters to circuit biases
                  \begin{itemize}
                  \item uses generic models for parameter translation routines
                        (e.g. circuit, synape)
                  \item does not require detailed knowledge of hardware
                  \item can optimize parameter search and evaluate
                        effectiveness of parameter translations
                  \item if desired configuration/state is reached maintain the
                        system
                  \item pyNCS: hardware interface and model abstraction
                  \item pyTune: defines high-lvl params as functions of high- or
                        low-lvl params and optimizes cost-functions with
                        experiment output
                  \end{itemize}
            \item high level formal description and automation of parameter
                  optimization problem
            \item pyNCS server-client architecture $\rightarrow$ multi-client
                  support, load sharing, remote access
            \item to integrate a new system, we need files describing:
                  \begin{itemize}
                  \item funcional blocks (neurons, E/I synapses, etc.)
                  \item setup (how many chips, which measurement instruments,
                        etc.)
                  \item topology of neural network
                  \item low- and high-level parameter mapping using hierachical
                        scheme
                  \end{itemize}
            \end{itemize}

        \subsection{Gao 2012: Dynamical System Guided Mapping of Quantitative
            Neuronal Models Onto Neuromorphic Hardware}
            \begin{itemize}
            \item Quantitative Mapping Method using dynamical system theory
                  \begin{enumerate}
                  \item calibrate on-chip bias generators on custom HW
                  \item estimate key mapping parameters (with set of linear
                        relationships with static input)
                  \end{enumerate}
            \item mathematical model analysis to determine linear relationship
                  \begin{itemize}
                  \item e.g. $f_{spike}(v_{input})$ non linear \& $v_{input}
                        (I_{bias1},I_{bias2})$ \& $I_{bias1}(\nu)\ I_{bias2}
                        (\nu)$, $\nu$ as mapping parameter
                  \item sweep currents, record spike rates, nonlinear fit of
                        analytical solution $\rightarrow$ naive, adhoc, unsafe
                  \item alternative: $v_{input}=0.5\curvearrowright$ bifurkation
                        $\curvearrowright$ bias currents constraints
                        $\curvearrowright$ obtain $\nu$ with linear least square
                        fit
                  \end{itemize}
            \item improve mapping throughput by exploiting high bandwidth spike
                  communication and through random and uniform subsampling
            \item experiment: QIAF model with bias generator and high-troughput
                  spike based mapping procedure onto 3 different chips
            \item throughput improved by over 16 fold
            \item Jenson-Shannon divergence reduced to 10\% between chips
                  (mapping method/shuffled control) and $\mu_{T_{ISI}} =3.4\%$
            \item large variations of mapping parameters between chips
                  (parameters B onto chip A results in high JS-div)
            \end{itemize}

        \subsection{Grassia 2011: Tunable neuromimetic integrated system for
            emulating cortical neuron models}
            \begin{itemize}
            \item full-custom fitting method in voltage-clamp mode on HH-Chip
                  "Galway"
            \item 
            \end{itemize}

        \subsection{Schwartz Dissertation 2011: Reproducing Biologically
            Realistic Regimes on a Highly-Accelerated Neuromorphic Hardware
            System}
            \begin{itemize}
            \item
            \end{itemize}

        \subsection{Brüderle Dissertation 2009: Neuroscientific Modeling with a
            Mixed-Signal VLSI Hardware System}
            \begin{itemize}
            \item
            \end{itemize}

        \subsection{Brüderle 2011: A Comprehensive Workflow for General-Purpose
            Neural Modeling with Highly Configurable Neuromorphic Hardware
            Systems}
            \begin{itemize}
            \item
            \end{itemize}

    \section{Mögliche wichtige Arbeiten}
        \begin{itemize}
        \item determining unknown parameters and state variables of physical
              systems by measurements of a limited number of observables
              (parameter estimation methods) from Neftci 2010
        \item Brillinger 2008
        \item Keat, Reinagel, Reid, Meister 2001
        \item Paninski, Pillow, Simoncelli 2004
        \item Okatan, Wilson, Brown 2005
        \item Huys, Ahrens, Paninski 2006
        \item Abarbanel, Creveling, Farsian, Kostuk 2009
        \end{itemize}

    \section{Notizen}
        \begin{itemize}
        \item Vorteile von neuromorpher Hardware gegenüber Softwaresimulation:
              \begin{itemize}
              \item Massiv Parallele Operationen
              \item Niedriger Energieverbrauch
              \item Verbesserte Skalierbarkeit
              \end{itemize}
        \item iterative Parametersuche kann durch Heuristik verbessert werden
              \cite{russell2007ConSpiCenPatGenNetBipWalUsiGenAlg}
        \item IAF sind strombasiert
        \item Das erste IAF Modell ist das Axon-Hillock Modell von Mead
        \item Das Axon-Hillock-Modell nutzt einen konstanten Strom als leak statt
              Konduktanz
        \item LTUs sind simple Modelle, die keine Spikezeitinformation und
              nichtlineare Elemente beinhalten
        \item der positive Feedback von CLIAF kann vernachlässigt werden, wenn
              der Threshold so gesetzt ist, das der ISI gleich bleibt
        \item man kann $\tau_{refrac}$ und Pulsweite ignorieren falls beide $\ll
              T_{ISI}$
        \item falls $\mean{T_{ISI}}\ll\tau_{syn}$ fluktuiert der Synapsenstrom um
              konstanten Wert $\sim w, f_{ISI}$
        \item Synapsen werden oft als Tiefpässe realisiert
        \item viele Annahmen bei Berechnungen $\rightarrow$ verringerte
              Komplexität $\rightarrow$ analystisch lösbar
        \item Nicht vorhandene Parameter mit nichtlinearen Abhängigkeiten müssen
              experimentell bestimmt werden
        \item Arten von Kalibration ver 1:
              \begin{enumerate}
              \item modellose brute-force Verhaltensanpassung
                    \cite{arthur2007SynSilGamRhy} (beschleunigt,Brüderle)
                    (heuristisch, ad-hoc)
              \item modelbasierte nichtlineare Optimierung (LTU-approximation,
                    Neftci)
              \item suche um manuell bestimmte Operating Points (Boahen)
              \item software (mismatch unbeachtet)
              \item hybride
              \end{enumerate}
        \item Arten von Kalibration ver 2:
              \begin{enumerate}
              \item modellos (adhoc)\cite{arthur2007SynSilGamRhy} vs
                    modellbasiert (ODE Synthetisierung, Zustandsvariablen als
                    Strombias)
              \item Approximierung zu einfachen Model (z.B. LTU
                    \cite{neftci2011SysMetConVLSNetSpiNeu})
              \item iterative Biasanpassung durch Intuition/Heuristik
                    \cite{arthur2007SynSilGamRhy}/nichtlineare
                    Optimierungsalgorithmen mit Softwaresimulationreferenz
                    \cite{sheik2011Sysconauttunneusys} (+ kompensiert
                    HWAbweichungen)
              \item mathematische Modelanalyse um lineare Beziehungen
                    festzustellen \cite{gao2012Dynsysguimapquaneumodontneuhar}
              \item suche um manuell bestimmte Operating Points (Boahen)
              \item software (mismatch unbeachtet)
              \end{enumerate}
        \item Fehlerarten: Hardware-Nichtlinearitäten, Transistor Mismatch,
              Kalibrationsungenauigkeiten,
        \item ca. 2010: automatisiertes VLSI Mapping noch heuristische Ad-hoc
              Kalibrationsroutinen
        \item Beispiel einer Kostenfunktion: $(r-r_{desired})^{2}$
        \item Jenson-Shannon Divergenz als Equivalenzgrad von Verteilungen (gut
              zum Vergleich zwischen Modell und Schaltung)
        \end{itemize}

    \section{Nützliche Abbildungen}
        \begin{figure}[H]
            \centering
            \includegraphics[width=1.00\textwidth]{figures/izhikevich_2004.png}
            \caption{Summary of neuro-computational properties exhibited by the
                     simple model (2.1)–(2.2). The figure is reproduced, with
                     permission, from www.izhikevich.com.
                     \cite{izhikevich2004WhiModtoUseCorSpiNeu_3}}
            \label{fig:spiking_summary}
        \end{figure}
        \begin{figure}[H]
            \centering
            \includegraphics[width=1.00\textwidth]{figures/izhikevich_2004_3.png}
            \caption{Comparison of the neuro-computational properties of spiking
                     and bursting models; Fig. \ref{fig:spiking_summary}. “\# of
                     FLOPS” is an approximate number of floating point operations
                     (addition, multiplication, etc.) needed to simulate the
                     model during a 1 ms time span. Each empty square indicates
                     the property that the model should exhibit in principle (in
                     theory) if the parameters are chosen appropriately, but the
                     author failed to find the parameters within a reasonable
                     period of time.\cite{izhikevich2004WhiModtoUseCorSpiNeu_3}}
            \label{fig:model_comparison}
        \end{figure}

    \section{Notizen zur Aufgabe}
        \begin{itemize}
        \item Das Wissen zur Hardware sprengt den Rahmen und Methoden von anderen
              Lehrstühlen basieren auf diesem Wissen (Sheik nicht ?!)
        \item Messprotokolle sollen HW-unabhängig sein
        \item Zustand mit Boundaries hervorrufen $\rightarrow$ Komplexität
              einschränken
        \end{itemize}

    \section{Kleine nützliche Wortsammlung}
        \begin{itemize}
        \item parameter estimation/extraction/translation/mapping
        \item quantitative link between parameters of NModels/electronic analogs
        \item parameter space
        \item population activity measurements
        \item abstract computational layer
        \item mean field approach
        \item context-dependent task

        \item derivative - Ableitung
        \item inferred - von etw. abgeleitet
        \item prohibitive - untragbar
        \item regime - Charakteristik
        \item coherently - folgerichtig, schlüssig, kohärent
        \item comprise - beinhalten

        \item cognition - Wahrnehmung, Erkennung
        \item Synthese - Zusammenführung zu einer Einheit oder künstliche
              Herstellung
        \end{itemize}

    \bibliographystyle{abbrvdin}
    \bibliography{referenzen}

%     \addcontentsline{toc}{section}{Glossary}
%     \printglossary
\end{document}
